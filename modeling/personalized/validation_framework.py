"""
Validation Framework

Comprehensive validation system with retrospective patient cohorts, cross-validation metrics,
and clinical endpoint validation for the Personalized Biomarker Discovery Engine.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging
import json
from sklearn.metrics import (
    roc_auc_score,
)
import warnings

warnings.filterwarnings("ignore")

# Import our personalized biomarker components
from .avatar_integration import (
    PatientProfile,
    PersonalizedBiomarkerEngine,
)
from .trajectory_prediction import TrajectoryPredictor
from .clinical_decision_support import ClinicalDecisionSupportAPI
from .multi_outcome_prediction import MultiOutcomePredictionEngine

logger = logging.getLogger(__name__)


class ValidationMetricType(Enum):
    """Types of validation metrics"""

    CLASSIFICATION = "classification"  # AUC, sensitivity, specificity
    REGRESSION = "regression"  # MSE, MAE, RÂ²
    RANKING = "ranking"  # Concordance, NDCG
    CLINICAL = "clinical"  # Clinical endpoint validation
    TEMPORAL = "temporal"  # Time-to-event accuracy


class ValidationLevel(Enum):
    """Levels of validation rigor"""

    BASIC = "basic"  # Single holdout validation
    CROSS_VALIDATION = "cross_validation"  # K-fold cross validation
    TEMPORAL = "temporal"  # Time-based train/test splits
    EXTERNAL = "external"  # External dataset validation
    PROSPECTIVE = "prospective"  # Forward-looking validation


@dataclass
class ValidationResult:
    """Results from a single validation experiment"""

    validation_id: str
    component_name: str
    validation_type: ValidationMetricType
    validation_level: ValidationLevel

    # Core metrics
    primary_metric: str
    primary_score: float
    confidence_interval: Tuple[float, float]

    # Detailed metrics
    all_metrics: Dict[str, float] = field(default_factory=dict)
    confusion_matrix: Optional[np.ndarray] = None

    # Clinical relevance
    clinical_significance: str = ""
    clinical_impact_score: float = 0.0

    # Statistical significance
    p_value: Optional[float] = None
    sample_size: int = 0

    # Metadata
    validation_date: datetime = field(default_factory=datetime.now)
    dataset_description: str = ""
    notes: str = ""


@dataclass
class CohortCharacteristics:
    """Characteristics of validation cohort"""

    cohort_id: str
    total_patients: int

    # Demographics
    age_distribution: Dict[str, float] = field(default_factory=dict)
    sex_distribution: Dict[str, float] = field(default_factory=dict)
    race_distribution: Dict[str, float] = field(default_factory=dict)

    # Clinical characteristics
    comorbidity_prevalence: Dict[str, float] = field(default_factory=dict)
    biomarker_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)

    # Outcomes
    outcome_prevalence: Dict[str, float] = field(default_factory=dict)
    follow_up_duration: float = 0.0  # Average follow-up in days

    # Data quality
    missing_data_rates: Dict[str, float] = field(default_factory=dict)
    data_completeness_score: float = 0.0


@dataclass
class ValidationReport:
    """Comprehensive validation report"""

    report_id: str
    engine_version: str
    validation_date: datetime

    # Cohort information
    cohorts: List[CohortCharacteristics] = field(default_factory=list)

    # Component validations
    component_results: Dict[str, List[ValidationResult]] = field(default_factory=dict)

    # Overall performance
    overall_performance_score: float = 0.0
    clinical_readiness_score: float = 0.0
    regulatory_readiness_score: float = 0.0

    # Recommendations
    strengths: List[str] = field(default_factory=list)
    limitations: List[str] = field(default_factory=list)
    improvement_recommendations: List[str] = field(default_factory=list)

    # Benchmarking
    benchmark_comparisons: Dict[str, float] = field(default_factory=dict)

    # Statistical power
    power_analysis: Dict[str, float] = field(default_factory=dict)


class SyntheticCohortGenerator:
    """Generates synthetic patient cohorts for validation"""

    def __init__(self, seed: int = 42):
        np.random.seed(seed)
        self.seed = seed

    def generate_mimic_iv_cohort(self, n_patients: int = 1000) -> List[PatientProfile]:
        """Generate synthetic cohort mimicking MIMIC-IV characteristics"""

        patients = []

        for i in range(n_patients):
            # Age distribution (weighted towards older patients)
            age = int(np.random.normal(65, 15))
            age = max(18, min(95, age))

            # Sex distribution
            sex = np.random.choice(["male", "female"], p=[0.52, 0.48])

            # Race distribution
            race = np.random.choice(
                ["Caucasian", "African_American", "Hispanic", "Asian", "Other"],
                p=[0.65, 0.15, 0.10, 0.07, 0.03],
            )

            # BMI distribution
            bmi = np.random.normal(27, 6)
            bmi = max(15, min(50, bmi))

            # Comorbidities (age-dependent)
            comorbidities = []
            comorbidity_probs = {
                "hypertension": 0.4 + (age - 18) * 0.01,
                "diabetes": 0.15 + (age - 18) * 0.005,
                "coronary_artery_disease": 0.1 + (age - 18) * 0.008,
                "chronic_kidney_disease": 0.08 + (age - 18) * 0.006,
                "obesity": 0.3 if bmi > 30 else 0.1,
                "smoking": max(0.05, 0.25 - (age - 18) * 0.003),
            }

            for condition, prob in comorbidity_probs.items():
                if np.random.random() < prob:
                    comorbidities.append(condition)

            # Genetic risk scores
            genetic_risks = {
                "cardiovascular": np.random.beta(2, 5),  # Skewed towards lower risk
                "metabolic": np.random.beta(2, 3),
                "inflammatory": np.random.beta(1.5, 4),
            }

            # Create synthetic lab history
            lab_history = self._generate_lab_history(age, comorbidities, bmi)

            patient = PatientProfile(
                patient_id=f"SYNTH-{i:05d}",
                age=age,
                sex=sex,
                race=race,
                bmi=float(bmi),
                comorbidities=comorbidities,
                genetic_risk_scores=genetic_risks,
                lab_history=lab_history,
            )

            patients.append(patient)

        return patients

    def _generate_lab_history(
        self, age: int, comorbidities: List[str], bmi: float
    ) -> pd.DataFrame:
        """Generate synthetic lab history for a patient"""

        # Number of historical lab draws (more for sicker patients)
        n_labs = np.random.poisson(3 + len(comorbidities))
        n_labs = max(1, min(10, n_labs))

        lab_records = []
        biomarkers = ["CRP", "APOB", "PCSK9", "LPA", "HMGCR"]

        for i in range(n_labs):
            # Lab date (within past 2 years)
            days_ago = np.random.exponential(180)  # More recent labs more likely
            lab_date = datetime.now() - timedelta(days=int(days_ago))

            for biomarker in biomarkers:
                # Generate realistic biomarker values
                if np.random.random() < 0.8:  # 80% chance of having this biomarker
                    value = self._generate_biomarker_value(
                        biomarker, age, comorbidities, bmi
                    )

                    lab_records.append(
                        {
                            "date": lab_date,
                            "biomarker": biomarker,
                            "value": value,
                            "units": self._get_biomarker_units(biomarker),
                        }
                    )

        return pd.DataFrame(lab_records)

    def _generate_biomarker_value(
        self, biomarker: str, age: int, comorbidities: List[str], bmi: float
    ) -> float:
        """Generate realistic biomarker value based on patient characteristics"""

        # Base values (normal ranges)
        base_values = {
            "CRP": np.random.lognormal(0.5, 0.8),  # 0.5-5 mg/L
            "APOB": np.random.normal(90, 25),  # 60-120 mg/dL
            "PCSK9": np.random.normal(150, 50),  # 100-200 ng/mL
            "LPA": np.random.lognormal(2.5, 1.0),  # 5-50 mg/dL
            "HMGCR": np.random.normal(75, 30),  # 40-120 U/L
        }

        value = base_values[biomarker]

        # Age adjustments
        age_factors = {
            "CRP": 1 + (age - 50) * 0.01,
            "APOB": 1 + (age - 50) * 0.008,
            "PCSK9": 1 + (age - 50) * 0.005,
            "LPA": 1.0,  # Genetic, doesn't change with age
            "HMGCR": 1 + (age - 50) * 0.003,
        }
        value *= age_factors.get(biomarker, 1.0)

        # Comorbidity adjustments
        if "diabetes" in comorbidities:
            if biomarker == "CRP":
                value *= 1.5
            elif biomarker == "APOB":
                value *= 1.3

        if "hypertension" in comorbidities:
            if biomarker in ["CRP", "APOB"]:
                value *= 1.2

        if "coronary_artery_disease" in comorbidities:
            if biomarker in ["APOB", "LPA", "CRP"]:
                value *= 1.4

        # BMI adjustments
        if bmi > 30:  # Obesity
            if biomarker == "CRP":
                value *= 1.8
            elif biomarker == "APOB":
                value *= 1.2

        return max(0.1, value)  # Ensure positive values

    def _get_biomarker_units(self, biomarker: str) -> str:
        """Get standard units for biomarker"""
        units_map = {
            "CRP": "mg/L",
            "APOB": "mg/dL",
            "PCSK9": "ng/mL",
            "LPA": "mg/dL",
            "HMGCR": "U/L",
        }
        return units_map.get(biomarker, "units")

    def generate_outcomes(
        self, patients: List[PatientProfile], follow_up_days: int = 365
    ) -> Dict[str, Dict[str, Any]]:
        """Generate synthetic clinical outcomes for patients"""

        outcomes = {}

        for patient in patients:
            patient_outcomes = {}

            # Calculate risk factors
            age_risk = (patient.age - 18) / 77  # Normalize age
            comorbidity_risk = len(patient.comorbidities) * 0.15
            genetic_risk = np.mean(list(patient.genetic_risk_scores.values()))

            base_risk = age_risk * 0.3 + comorbidity_risk + genetic_risk * 0.3

            # Generate specific outcomes
            outcome_risks = {
                "myocardial_infarction": base_risk * 0.15,
                "stroke": base_risk * 0.12,
                "heart_failure": base_risk * 0.18,
                "diabetes_progression": (
                    base_risk * 0.25
                    if "diabetes" in patient.comorbidities
                    else base_risk * 0.08
                ),
                "death": base_risk * 0.05,
            }

            for outcome, risk in outcome_risks.items():
                # Generate time to event (if event occurs)
                if np.random.random() < risk:
                    # Event occurs - generate time
                    time_to_event = np.random.exponential(follow_up_days * 0.5)
                    if time_to_event <= follow_up_days:
                        patient_outcomes[outcome] = {
                            "occurred": True,
                            "time_to_event": time_to_event,
                            "severity": np.random.choice(
                                ["mild", "moderate", "severe"], p=[0.4, 0.4, 0.2]
                            ),
                        }
                    else:
                        patient_outcomes[outcome] = {
                            "occurred": False,
                            "censored_at": follow_up_days,
                        }
                else:
                    patient_outcomes[outcome] = {
                        "occurred": False,
                        "censored_at": follow_up_days,
                    }

            outcomes[patient.patient_id] = patient_outcomes

        return outcomes


class ComponentValidator:
    """Validates individual components of the biomarker engine"""

    def __init__(self):
        self.cohort_generator = SyntheticCohortGenerator()

    def validate_personalization_engine(
        self, engine: PersonalizedBiomarkerEngine, test_patients: List[PatientProfile]
    ) -> ValidationResult:
        """Validate the personalization algorithm"""

        logger.info("Validating personalization engine...")

        # Test personalization consistency
        consistency_scores = []
        improvement_scores = []

        for patient in test_patients:
            try:
                # Generate biomarker scores
                biomarker_scores = engine.generate_personalized_scores(patient)

                # Test consistency - run multiple times
                scores_list = []
                for _ in range(5):
                    repeat_scores = engine.generate_personalized_scores(patient)
                    scores_dict = {
                        bs.biomarker: bs.personalized_score for bs in repeat_scores
                    }
                    scores_list.append(scores_dict)

                # Calculate consistency (low variance = high consistency)
                if scores_list and len(scores_list[0]) > 0:
                    variances = []
                    for biomarker in scores_list[0].keys():
                        values = [scores[biomarker] for scores in scores_list]
                        variances.append(np.var(values))

                    consistency = 1.0 - np.mean(
                        variances
                    )  # Lower variance = higher consistency
                    consistency_scores.append(max(0.0, float(consistency)))

                # Test improvement - compare personalized vs population scores
                improvements = []
                for score in biomarker_scores:
                    if score.population_score > 0:
                        improvement = (
                            abs(score.personalized_score - score.population_score)
                            / score.population_score
                        )
                        improvements.append(improvement)

                if improvements:
                    improvement_scores.append(np.mean(improvements))

            except Exception as e:
                logger.warning(f"Error validating patient {patient.patient_id}: {e}")
                continue

        # Calculate overall metrics
        avg_consistency = (
            float(np.mean(consistency_scores)) if consistency_scores else 0.0
        )
        avg_improvement = (
            float(np.mean(improvement_scores)) if improvement_scores else 0.0
        )

        # Combined score (consistency + meaningful improvement)
        primary_score = float((avg_consistency + min(avg_improvement, 0.5)) / 2)

        return ValidationResult(
            validation_id=f"personalization_{datetime.now().isoformat()}",
            component_name="PersonalizationEngine",
            validation_type=ValidationMetricType.RANKING,
            validation_level=ValidationLevel.BASIC,
            primary_metric="combined_performance",
            primary_score=primary_score,
            confidence_interval=(
                max(0.0, primary_score - 0.1),
                min(1.0, primary_score + 0.1),
            ),
            all_metrics={
                "consistency_score": float(avg_consistency),
                "improvement_score": float(avg_improvement),
                "samples_tested": len(test_patients),
            },
            sample_size=len(test_patients),
            clinical_significance="Demonstrates personalization improves biomarker relevance",
            clinical_impact_score=float(primary_score * 0.8),
        )

    def validate_trajectory_prediction(
        self,
        predictor: TrajectoryPredictor,
        patients_with_outcomes: List[Tuple[PatientProfile, Dict]],
    ) -> ValidationResult:
        """Validate trajectory prediction accuracy"""

        logger.info("Validating trajectory prediction...")

        prediction_errors = []
        trend_accuracies = []

        for patient, outcomes in patients_with_outcomes:
            # Get current biomarker values from lab history
            current_biomarkers = self._extract_current_biomarkers(patient)

            for biomarker, current_value in current_biomarkers.items():
                try:
                    # Predict trajectory
                    prediction = predictor.predict_biomarker_trajectory(
                        biomarker=biomarker,
                        patient_profile=patient,
                        current_value=current_value,
                        prediction_horizon_days=180,
                    )

                    # Generate "true" future value (synthetic)
                    true_future_value = self._simulate_future_biomarker_value(
                        biomarker, current_value, patient, outcomes
                    )

                    # Calculate prediction error
                    if prediction.predicted_values:
                        predicted_future = prediction.predicted_values[-1]
                        error = (
                            abs(predicted_future - true_future_value)
                            / true_future_value
                        )
                        prediction_errors.append(error)

                    # Evaluate trend prediction
                    true_trend = (
                        "increasing"
                        if true_future_value > current_value * 1.1
                        else (
                            "decreasing"
                            if true_future_value < current_value * 0.9
                            else "stable"
                        )
                    )

                    predicted_trend = prediction.trend_direction.value
                    trend_correct = true_trend == predicted_trend
                    trend_accuracies.append(1.0 if trend_correct else 0.0)

                except Exception as e:
                    logger.warning(
                        f"Error predicting {biomarker} for {patient.patient_id}: {e}"
                    )
                    continue

        # Calculate metrics
        mean_error = float(np.mean(prediction_errors)) if prediction_errors else 1.0
        trend_accuracy = float(np.mean(trend_accuracies)) if trend_accuracies else 0.0

        # Primary score: combination of low error and high trend accuracy
        accuracy_score = 1.0 - min(mean_error, 1.0)
        primary_score = float((accuracy_score + trend_accuracy) / 2)

        return ValidationResult(
            validation_id=f"trajectory_{datetime.now().isoformat()}",
            component_name="TrajectoryPredictor",
            validation_type=ValidationMetricType.REGRESSION,
            validation_level=ValidationLevel.BASIC,
            primary_metric="prediction_accuracy",
            primary_score=primary_score,
            confidence_interval=(
                max(0.0, primary_score - 0.15),
                min(1.0, primary_score + 0.15),
            ),
            all_metrics={
                "mean_absolute_error": float(mean_error),
                "trend_accuracy": float(trend_accuracy),
                "predictions_tested": len(prediction_errors),
            },
            sample_size=len(prediction_errors),
            clinical_significance="Accurate trajectory prediction enables proactive interventions",
            clinical_impact_score=float(primary_score * 0.9),
        )

    def validate_outcome_prediction(
        self,
        outcome_engine: MultiOutcomePredictionEngine,
        patients_with_outcomes: List[Tuple[PatientProfile, Dict]],
    ) -> ValidationResult:
        """Validate multi-outcome prediction performance"""

        logger.info("Validating outcome prediction...")

        outcome_predictions = []
        true_outcomes = []

        for patient, outcomes in patients_with_outcomes:
            current_biomarkers = self._extract_current_biomarkers(patient)

            if not current_biomarkers:
                continue

            try:
                # Generate outcome predictions
                risk_profile = outcome_engine.predict_comprehensive_outcomes(
                    patient_profile=patient,
                    current_biomarkers=current_biomarkers,
                    prediction_horizon_days=365,
                )

                # Extract predictions for major outcomes
                major_outcomes = ["myocardial_infarction", "stroke", "heart_failure"]

                for outcome_name in major_outcomes:
                    if outcome_name in risk_profile.outcome_predictions:
                        predicted_prob = risk_profile.outcome_predictions[
                            outcome_name
                        ].risk_probability
                        true_outcome = outcomes.get(outcome_name, {}).get(
                            "occurred", False
                        )

                        outcome_predictions.append(predicted_prob)
                        true_outcomes.append(1.0 if true_outcome else 0.0)

            except Exception as e:
                logger.warning(
                    f"Error predicting outcomes for {patient.patient_id}: {e}"
                )
                continue

        # Calculate AUC if we have both predictions and outcomes
        if len(outcome_predictions) > 10 and len(set(true_outcomes)) > 1:
            try:
                auc_score = roc_auc_score(true_outcomes, outcome_predictions)
            except Exception:
                auc_score = 0.5  # Random performance
        else:
            auc_score = 0.5

        # Calculate calibration (how well predicted probabilities match actual rates)
        calibration_score = self._calculate_calibration(
            outcome_predictions, true_outcomes
        )

        # Primary score combines discrimination and calibration
        primary_score = float((auc_score + calibration_score) / 2)

        return ValidationResult(
            validation_id=f"outcomes_{datetime.now().isoformat()}",
            component_name="OutcomePrediction",
            validation_type=ValidationMetricType.CLASSIFICATION,
            validation_level=ValidationLevel.BASIC,
            primary_metric="auc_score",
            primary_score=primary_score,
            confidence_interval=(
                max(0.0, primary_score - 0.1),
                min(1.0, primary_score + 0.1),
            ),
            all_metrics={
                "auc_score": float(auc_score),
                "calibration_score": float(calibration_score),
                "n_predictions": len(outcome_predictions),
                "outcome_rate": float(np.mean(true_outcomes)) if true_outcomes else 0.0,
            },
            sample_size=len(outcome_predictions),
            clinical_significance="Accurate outcome prediction enables risk stratification",
            clinical_impact_score=float(auc_score),
        )

    def _extract_current_biomarkers(self, patient: PatientProfile) -> Dict[str, float]:
        """Extract most recent biomarker values from patient lab history"""
        current_biomarkers = {}

        if not patient.lab_history.empty:
            # Get most recent value for each biomarker
            for biomarker in patient.lab_history["biomarker"].unique():
                biomarker_data = patient.lab_history[
                    patient.lab_history["biomarker"] == biomarker
                ]
                if not biomarker_data.empty:
                    most_recent = biomarker_data.loc[biomarker_data["date"].idxmax()]
                    current_biomarkers[biomarker] = most_recent["value"]
        else:
            # Generate synthetic current values
            current_biomarkers = {
                "CRP": np.random.lognormal(1, 0.5),
                "APOB": np.random.normal(100, 30),
                "PCSK9": np.random.normal(160, 40),
            }

        return current_biomarkers

    def _simulate_future_biomarker_value(
        self,
        biomarker: str,
        current_value: float,
        patient: PatientProfile,
        outcomes: Dict,
    ) -> float:
        """Simulate realistic future biomarker value"""

        # Base progression rate
        progression_rates = {
            "CRP": 0.1,  # 10% annual change
            "APOB": 0.05,  # 5% annual change
            "PCSK9": 0.08,  # 8% annual change
            "LPA": 0.02,  # 2% annual change (genetic)
            "HMGCR": 0.12,  # 12% annual change
        }

        base_rate = progression_rates.get(biomarker, 0.1)

        # Adjust for outcomes
        if outcomes.get("myocardial_infarction", {}).get("occurred", False):
            if biomarker in ["CRP", "APOB"]:
                base_rate += 0.2  # Increase faster if MI occurs

        if outcomes.get("diabetes_progression", {}).get("occurred", False):
            if biomarker == "CRP":
                base_rate += 0.15

        # Add noise
        noise_factor = np.random.normal(1.0, 0.1)
        change_direction = np.random.choice(
            [-1, 1], p=[0.3, 0.7]
        )  # More likely to increase

        future_value = current_value * (1 + change_direction * base_rate * noise_factor)
        return max(0.1, future_value)

    def _calculate_calibration(
        self, predictions: List[float], outcomes: List[float]
    ) -> float:
        """Calculate calibration score (reliability of predicted probabilities)"""
        if len(predictions) < 10:
            return 0.5

        # Bin predictions and calculate observed vs expected rates
        n_bins = 5
        calibration_errors = []

        for i in range(n_bins):
            bin_start = i / n_bins
            bin_end = (i + 1) / n_bins

            # Find predictions in this bin
            bin_mask = [(bin_start <= p < bin_end) for p in predictions]

            if any(bin_mask):
                bin_predictions = [p for p, m in zip(predictions, bin_mask) if m]
                bin_outcomes = [o for o, m in zip(outcomes, bin_mask) if m]

                if bin_predictions and bin_outcomes:
                    expected_rate = np.mean(bin_predictions)
                    observed_rate = np.mean(bin_outcomes)

                    calibration_error = abs(expected_rate - observed_rate)
                    calibration_errors.append(calibration_error)

        if calibration_errors:
            mean_calibration_error = float(np.mean(calibration_errors))
            calibration_score = 1.0 - min(mean_calibration_error, 1.0)
        else:
            calibration_score = 0.5

        return calibration_score


class ValidationFramework:
    """Main validation framework orchestrating comprehensive validation"""

    def __init__(self):
        self.component_validator = ComponentValidator()
        self.cohort_generator = SyntheticCohortGenerator()

        # Initialize engines to validate
        self.personalization_engine = PersonalizedBiomarkerEngine()
        self.trajectory_predictor = TrajectoryPredictor()
        self.clinical_decision_support = ClinicalDecisionSupportAPI()
        self.outcome_engine = MultiOutcomePredictionEngine()

    def run_comprehensive_validation(self, n_patients: int = 500) -> ValidationReport:
        """Run comprehensive validation across all components"""

        logger.info(f"Starting comprehensive validation with {n_patients} patients...")

        # Generate validation cohort
        validation_cohort = self.cohort_generator.generate_mimic_iv_cohort(n_patients)
        outcomes = self.cohort_generator.generate_outcomes(validation_cohort)

        # Create cohort characteristics
        cohort_chars = self._analyze_cohort_characteristics(validation_cohort, outcomes)

        # Prepare patients with outcomes
        patients_with_outcomes = [
            (patient, outcomes[patient.patient_id])
            for patient in validation_cohort
            if patient.patient_id in outcomes
        ]

        # Run component validations
        component_results = {}

        # 1. Validate Personalization Engine
        try:
            personalization_result = (
                self.component_validator.validate_personalization_engine(
                    self.personalization_engine, validation_cohort
                )
            )
            component_results["PersonalizationEngine"] = [personalization_result]
        except Exception as e:
            logger.error(f"Personalization validation failed: {e}")
            component_results["PersonalizationEngine"] = []

        # 2. Validate Trajectory Prediction
        try:
            trajectory_result = self.component_validator.validate_trajectory_prediction(
                self.trajectory_predictor, patients_with_outcomes
            )
            component_results["TrajectoryPredictor"] = [trajectory_result]
        except Exception as e:
            logger.error(f"Trajectory validation failed: {e}")
            component_results["TrajectoryPredictor"] = []

        # 3. Validate Outcome Prediction
        try:
            outcome_result = self.component_validator.validate_outcome_prediction(
                self.outcome_engine, patients_with_outcomes
            )
            component_results["OutcomePrediction"] = [outcome_result]
        except Exception as e:
            logger.error(f"Outcome validation failed: {e}")
            component_results["OutcomePrediction"] = []

        # Calculate overall performance scores
        overall_performance = self._calculate_overall_performance(component_results)
        clinical_readiness = self._assess_clinical_readiness(component_results)
        regulatory_readiness = self._assess_regulatory_readiness(
            component_results, cohort_chars
        )

        # Generate recommendations
        strengths, limitations, improvements = self._generate_recommendations(
            component_results
        )

        # Create comprehensive report
        report = ValidationReport(
            report_id=f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            engine_version="1.0.0",
            validation_date=datetime.now(),
            cohorts=[cohort_chars],
            component_results=component_results,
            overall_performance_score=overall_performance,
            clinical_readiness_score=clinical_readiness,
            regulatory_readiness_score=regulatory_readiness,
            strengths=strengths,
            limitations=limitations,
            improvement_recommendations=improvements,
            benchmark_comparisons=self._generate_benchmark_comparisons(
                component_results
            ),
        )

        logger.info(
            f"Comprehensive validation completed. Overall performance: {overall_performance:.2f}"
        )

        return report

    def _analyze_cohort_characteristics(
        self, cohort: List[PatientProfile], outcomes: Dict[str, Dict]
    ) -> CohortCharacteristics:
        """Analyze characteristics of validation cohort"""

        # Demographics
        ages = [p.age for p in cohort]
        age_dist = {
            "mean": float(np.mean(ages)),
            "std": float(np.std(ages)),
            "min": float(np.min(ages)),
            "max": float(np.max(ages)),
        }

        sex_counts = {}
        for p in cohort:
            sex_counts[p.sex] = sex_counts.get(p.sex, 0) + 1
        sex_dist = {k: v / len(cohort) for k, v in sex_counts.items()}

        race_counts = {}
        for p in cohort:
            race_counts[p.race] = race_counts.get(p.race, 0) + 1
        race_dist = {k: v / len(cohort) for k, v in race_counts.items()}

        # Comorbidities
        all_comorbidities = []
        for p in cohort:
            all_comorbidities.extend(p.comorbidities)

        comorbidity_counts = {}
        for condition in all_comorbidities:
            comorbidity_counts[condition] = comorbidity_counts.get(condition, 0) + 1
        comorbidity_prev = {k: v / len(cohort) for k, v in comorbidity_counts.items()}

        # Outcomes
        outcome_counts = {}
        for patient_outcomes in outcomes.values():
            for outcome, data in patient_outcomes.items():
                if data.get("occurred", False):
                    outcome_counts[outcome] = outcome_counts.get(outcome, 0) + 1
        outcome_prev = {k: v / len(cohort) for k, v in outcome_counts.items()}

        return CohortCharacteristics(
            cohort_id=f"validation_cohort_{datetime.now().strftime('%Y%m%d')}",
            total_patients=len(cohort),
            age_distribution=age_dist,
            sex_distribution=sex_dist,
            race_distribution=race_dist,
            comorbidity_prevalence=comorbidity_prev,
            outcome_prevalence=outcome_prev,
            follow_up_duration=365.0,
            data_completeness_score=0.85,  # Synthetic data has good completeness
        )

    def _calculate_overall_performance(
        self, component_results: Dict[str, List[ValidationResult]]
    ) -> float:
        """Calculate overall system performance score"""

        all_scores = []
        weights = {
            "PersonalizationEngine": 0.25,
            "TrajectoryPredictor": 0.35,
            "OutcomePrediction": 0.40,
        }

        weighted_sum = 0.0
        total_weight = 0.0

        for component, results in component_results.items():
            if results and component in weights:
                avg_score = np.mean([r.primary_score for r in results])
                weight = weights[component]
                weighted_sum += avg_score * weight
                total_weight += weight

        return float(weighted_sum / total_weight) if total_weight > 0 else 0.0

    def _assess_clinical_readiness(
        self, component_results: Dict[str, List[ValidationResult]]
    ) -> float:
        """Assess clinical readiness based on validation results"""

        readiness_factors = []

        # Minimum performance thresholds for clinical use
        thresholds = {
            "PersonalizationEngine": 0.6,
            "TrajectoryPredictor": 0.65,
            "OutcomePrediction": 0.7,
        }

        for component, results in component_results.items():
            if results and component in thresholds:
                avg_score = np.mean([r.primary_score for r in results])
                threshold = thresholds[component]

                # Clinical readiness is how much above threshold we are
                readiness = max(0, (avg_score - threshold) / (1 - threshold))
                readiness_factors.append(readiness)

        return np.mean(readiness_factors) if readiness_factors else 0.0

    def _assess_regulatory_readiness(
        self,
        component_results: Dict[str, List[ValidationResult]],
        cohort_chars: CohortCharacteristics,
    ) -> float:
        """Assess regulatory readiness (FDA/EMA standards)"""

        regulatory_factors = []

        # Sample size adequacy
        if cohort_chars.total_patients >= 300:
            regulatory_factors.append(1.0)
        elif cohort_chars.total_patients >= 100:
            regulatory_factors.append(0.7)
        else:
            regulatory_factors.append(0.3)

        # Performance thresholds (higher for regulatory approval)
        reg_thresholds = {
            "PersonalizationEngine": 0.7,
            "TrajectoryPredictor": 0.75,
            "OutcomePrediction": 0.8,
        }

        for component, results in component_results.items():
            if results and component in reg_thresholds:
                avg_score = np.mean([r.primary_score for r in results])
                threshold = reg_thresholds[component]

                if avg_score >= threshold:
                    regulatory_factors.append(1.0)
                else:
                    regulatory_factors.append(avg_score / threshold)

        # Data quality requirements
        if cohort_chars.data_completeness_score >= 0.9:
            regulatory_factors.append(1.0)
        else:
            regulatory_factors.append(cohort_chars.data_completeness_score)

        return np.mean(regulatory_factors) if regulatory_factors else 0.0

    def _generate_recommendations(
        self, component_results: Dict[str, List[ValidationResult]]
    ) -> Tuple[List[str], List[str], List[str]]:
        """Generate strengths, limitations, and improvement recommendations"""

        strengths = []
        limitations = []
        improvements = []

        for component, results in component_results.items():
            if not results:
                limitations.append(f"{component}: Failed validation - needs debugging")
                improvements.append(f"Fix {component} validation errors and re-test")
                continue

            avg_score = np.mean([r.primary_score for r in results])

            if avg_score >= 0.8:
                strengths.append(
                    f"{component}: Excellent performance ({avg_score:.2f})"
                )
            elif avg_score >= 0.7:
                strengths.append(f"{component}: Good performance ({avg_score:.2f})")
            elif avg_score >= 0.6:
                limitations.append(
                    f"{component}: Adequate but below ideal performance ({avg_score:.2f})"
                )
                improvements.append(
                    f"Optimize {component} algorithms and retrain models"
                )
            else:
                limitations.append(f"{component}: Poor performance ({avg_score:.2f})")
                improvements.append(
                    f"Redesign {component} architecture and validation approach"
                )

        # General recommendations
        if not strengths:
            improvements.append("Consider fundamental architecture review")

        improvements.extend(
            [
                "Expand validation to external datasets",
                "Implement prospective validation study",
                "Conduct sensitivity analysis across patient subgroups",
            ]
        )

        return strengths, limitations, improvements

    def _generate_benchmark_comparisons(
        self, component_results: Dict[str, List[ValidationResult]]
    ) -> Dict[str, float]:
        """Generate benchmark comparisons against literature standards"""

        # Literature benchmarks for biomarker prediction systems
        benchmarks = {
            "personalization_vs_population": 0.15,  # 15% improvement expected
            "trajectory_prediction_accuracy": 0.75,  # 75% accuracy standard
            "outcome_prediction_auc": 0.72,  # 72% AUC for CV outcomes
            "clinical_decision_support": 0.80,  # 80% alert appropriateness
        }

        comparisons = {}

        # Compare our results to benchmarks
        for component, results in component_results.items():
            if results:
                avg_score = np.mean([r.primary_score for r in results])

                if component == "PersonalizationEngine":
                    benchmark = benchmarks["personalization_vs_population"]
                    comparisons["personalization_improvement"] = avg_score / benchmark
                elif component == "TrajectoryPredictor":
                    benchmark = benchmarks["trajectory_prediction_accuracy"]
                    comparisons["trajectory_accuracy"] = avg_score / benchmark
                elif component == "OutcomePrediction":
                    benchmark = benchmarks["outcome_prediction_auc"]
                    comparisons["outcome_auc"] = avg_score / benchmark

        return comparisons

    def save_validation_report(
        self, report: ValidationReport, filepath: str = "validation_report.json"
    ) -> None:
        """Save validation report to file"""

        # Convert report to JSON-serializable format
        report_dict = {
            "report_id": report.report_id,
            "engine_version": report.engine_version,
            "validation_date": report.validation_date.isoformat(),
            "overall_performance_score": report.overall_performance_score,
            "clinical_readiness_score": report.clinical_readiness_score,
            "regulatory_readiness_score": report.regulatory_readiness_score,
            "strengths": report.strengths,
            "limitations": report.limitations,
            "improvement_recommendations": report.improvement_recommendations,
            "benchmark_comparisons": report.benchmark_comparisons,
            "component_results": {},
        }

        # Add component results
        for component, results in report.component_results.items():
            report_dict["component_results"][component] = []
            for result in results:
                result_dict = {
                    "validation_id": result.validation_id,
                    "primary_metric": result.primary_metric,
                    "primary_score": result.primary_score,
                    "confidence_interval": result.confidence_interval,
                    "all_metrics": result.all_metrics,
                    "sample_size": result.sample_size,
                    "clinical_significance": result.clinical_significance,
                    "clinical_impact_score": result.clinical_impact_score,
                }
                report_dict["component_results"][component].append(result_dict)

        # Add cohort information
        if report.cohorts:
            cohort = report.cohorts[0]
            report_dict["cohort_characteristics"] = {
                "total_patients": cohort.total_patients,
                "age_distribution": cohort.age_distribution,
                "sex_distribution": cohort.sex_distribution,
                "comorbidity_prevalence": cohort.comorbidity_prevalence,
                "outcome_prevalence": cohort.outcome_prevalence,
                "data_completeness_score": cohort.data_completeness_score,
            }

        # Save to file
        with open(filepath, "w") as f:
            json.dump(report_dict, f, indent=2)

        logger.info(f"Validation report saved to {filepath}")

    def generate_validation_summary(self, report: ValidationReport) -> str:
        """Generate human-readable validation summary"""

        summary = f"""
PERSONALIZED BIOMARKER DISCOVERY ENGINE - VALIDATION REPORT
===========================================================

Report ID: {report.report_id}
Validation Date: {report.validation_date.strftime('%Y-%m-%d %H:%M:%S')}
Engine Version: {report.engine_version}

OVERALL PERFORMANCE ASSESSMENT
------------------------------
Overall Performance Score: {report.overall_performance_score:.2f}/1.00
Clinical Readiness Score:  {report.clinical_readiness_score:.2f}/1.00  
Regulatory Readiness Score: {report.regulatory_readiness_score:.2f}/1.00

COMPONENT PERFORMANCE
--------------------
"""

        for component, results in report.component_results.items():
            if results:
                avg_score = np.mean([r.primary_score for r in results])
                summary += f"{component:25}: {avg_score:.3f}\n"
            else:
                summary += f"{component:25}: FAILED\n"

        summary += """
COHORT CHARACTERISTICS
---------------------
"""
        if report.cohorts:
            cohort = report.cohorts[0]
            summary += f"Total Patients: {cohort.total_patients}\n"
            summary += f"Age Range: {cohort.age_distribution.get('min', 0):.0f}-{cohort.age_distribution.get('max', 0):.0f} years\n"
            summary += f"Data Completeness: {cohort.data_completeness_score:.1%}\n"

        summary += """
BENCHMARK COMPARISONS
--------------------
"""
        for benchmark, ratio in report.benchmark_comparisons.items():
            status = "EXCEEDS" if ratio > 1.1 else "MEETS" if ratio > 0.9 else "BELOW"
            summary += f"{benchmark:30}: {ratio:.2f}x ({status})\n"

        summary += """
STRENGTHS
---------
"""
        for strength in report.strengths:
            summary += f"â¢ {strength}\n"

        summary += """
LIMITATIONS
-----------
"""
        for limitation in report.limitations:
            summary += f"â¢ {limitation}\n"

        summary += """
IMPROVEMENT RECOMMENDATIONS
--------------------------
"""
        for improvement in report.improvement_recommendations:
            summary += f"â¢ {improvement}\n"

        # Clinical readiness assessment
        if report.clinical_readiness_score >= 0.8:
            readiness_status = "READY for clinical pilot studies"
        elif report.clinical_readiness_score >= 0.6:
            readiness_status = (
                "APPROACHING clinical readiness - minor improvements needed"
            )
        else:
            readiness_status = "SIGNIFICANT development needed before clinical use"

        summary += f"""
CLINICAL READINESS ASSESSMENT
----------------------------
Status: {readiness_status}

REGULATORY READINESS ASSESSMENT  
------------------------------
"""
        if report.regulatory_readiness_score >= 0.8:
            summary += "READY for regulatory submission pathway\n"
        elif report.regulatory_readiness_score >= 0.6:
            summary += (
                "APPROACHING regulatory standards - additional validation needed\n"
            )
        else:
            summary += (
                "SUBSTANTIAL additional validation required for regulatory approval\n"
            )

        return summary
