"""
Scaled Real Biomarker Data Integration
Handles full MIMIC-IV cohort with temporal dynamics and multiple outcomes
"""

from pathlib import Path
from typing import Dict, List, Optional, Any
import pandas as pd
import numpy as np
import json
from datetime import datetime
import multiprocessing as mp
import logging
from dataclasses import dataclass

from biomarkers.causal_scoring import CausalBiomarkerScorer

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class CohortConfiguration:
    """Configuration for scaled cohort analysis"""

    min_subjects: int = 1000
    max_subjects: Optional[int] = None
    min_biomarkers: int = 10
    temporal_window_hours: int = 48
    outcome_prediction_hours: int = 24
    include_temporal_features: bool = True
    parallel_processing: bool = True
    max_workers: Optional[int] = None
    chunk_size: int = 500
    cache_results: bool = True


class ScaledBiomarkerDataConnector:
    """
    Scaled version for full MIMIC-IV cohort analysis with temporal dynamics
    """

    def __init__(
        self,
        mimic_dir: Optional[Path] = None,
        config: Optional[CohortConfiguration] = None,
    ):
        self.mimic_dir = mimic_dir or Path("data/mimic")
        self.processed_dir = Path("data/processed")
        self.cache_dir = Path("data/cache/scaled_analysis")
        self.modules_file = Path("modeling/modules/tubular_modules_v1.json")
        self.config = config or CohortConfiguration()

        # Create cache directory
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Set up parallel processing
        if self.config.max_workers is None:
            self.config.max_workers = max(1, mp.cpu_count() - 1)

    def load_full_mimic_cohort(self) -> Optional[pd.DataFrame]:
        """
        Load full MIMIC-IV cohort with all available biomarkers
        """
        logger.info("üè• Loading full MIMIC-IV cohort...")

        # Check cache first
        cache_file = self.cache_dir / "full_cohort_features.parquet"
        if cache_file.exists() and self.config.cache_results:
            logger.info(f"   üì¶ Loading cached cohort: {cache_file}")
            try:
                cohort_df = pd.read_parquet(cache_file)
                logger.info(f"   ‚úÖ Loaded {len(cohort_df)} subjects from cache")
                return cohort_df
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Cache load failed: {e}")

        # Try to load from multiple sources
        data_sources = [
            self.processed_dir / "features.parquet",
            self.processed_dir / "labs_clean.parquet",
            Path("outputs/predictions/assoc.parquet"),
            self.processed_dir / "mimic_features_full.parquet",
        ]

        cohort_df = None
        for source in data_sources:
            if source.exists():
                logger.info(f"   üìä Loading from: {source}")
                try:
                    df = pd.read_parquet(source)
                    biomarker_cols = self._identify_biomarker_columns(df)

                    if len(biomarker_cols) >= self.config.min_biomarkers:
                        # Ensure subject ID column
                        if "subject_id" not in df.columns:
                            if "hadm_id" in df.columns:
                                df = self._aggregate_by_admission(df)
                            else:
                                df["subject_id"] = range(len(df))

                        cohort_df = df
                        logger.info(
                            f"   ‚úÖ Found {len(df)} subjects with {len(biomarker_cols)} biomarkers"
                        )
                        break
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Failed to load {source}: {e}")

        # If no adequate data found, create expanded synthetic cohort
        if cohort_df is None or len(cohort_df) < self.config.min_subjects:
            logger.info("   üß™ Creating expanded synthetic cohort...")
            cohort_df = self._create_expanded_synthetic_cohort()

        # Cache the result
        if self.config.cache_results and cohort_df is not None:
            try:
                cohort_df.to_parquet(cache_file)
                logger.info(f"   üíæ Cached cohort to: {cache_file}")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Failed to cache: {e}")

        return cohort_df

    def _aggregate_by_admission(self, df: pd.DataFrame) -> pd.DataFrame:
        """Aggregate data by admission ID to create subject-level features"""

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(exclude=[np.number]).columns

        # Aggregate numeric columns
        agg_funcs = {}
        for col in numeric_cols:
            if col != "hadm_id":
                agg_funcs[col] = ["mean", "min", "max", "std"]

        for col in categorical_cols:
            if col not in ["hadm_id"]:
                agg_funcs[col] = "first"

        aggregated = df.groupby("hadm_id").agg(agg_funcs).reset_index()

        # Flatten column names
        new_columns = []
        for col in aggregated.columns:
            if isinstance(col, tuple):
                if col[1]:  # Has aggregation function
                    new_columns.append(f"{col[0]}_{col[1]}")
                else:
                    new_columns.append(col[0])
            else:
                new_columns.append(col)

        aggregated.columns = new_columns
        aggregated.rename(columns={"hadm_id": "subject_id"}, inplace=True)

        return aggregated

    def _create_expanded_synthetic_cohort(self) -> pd.DataFrame:
        """Create large synthetic cohort for scaling demonstration"""

        n_subjects = max(self.config.min_subjects, 2000)
        logger.info(f"   üî¨ Generating {n_subjects} synthetic subjects...")

        np.random.seed(42)

        # Create realistic patient demographics
        cohort_data = {
            "subject_id": [f"scaled_subj_{i:06d}" for i in range(n_subjects)],
            "age": np.random.normal(65, 15, n_subjects).clip(18, 95),
            "gender": np.random.choice(["M", "F"], n_subjects),
            "admission_type": np.random.choice(
                ["ELECTIVE", "EMERGENCY", "URGENT"], n_subjects, p=[0.3, 0.5, 0.2]
            ),
        }

        # Generate correlated clinical biomarkers
        base_risk = np.random.beta(
            2, 8, n_subjects
        )  # Most patients low risk, some high risk

        # Core kidney biomarkers
        creatinine_baseline = np.random.normal(1.0, 0.3, n_subjects).clip(0.5, 3.0)
        aki_risk = base_risk + (creatinine_baseline - 1.0) / 2

        # Time series features (48-hour window)
        for biomarker in ["creatinine", "urea", "potassium", "sodium", "chloride"]:
            for stat in ["min", "max", "mean", "std", "slope_24h", "peak_time"]:
                if biomarker == "creatinine":
                    if stat == "mean":
                        values = creatinine_baseline * (1 + aki_risk * 0.8)
                    elif stat == "max":
                        values = creatinine_baseline * (1 + aki_risk * 1.5)
                    elif stat == "min":
                        values = creatinine_baseline * (1 + aki_risk * 0.2)
                    elif stat == "std":
                        values = aki_risk * 0.5
                    elif stat == "slope_24h":
                        values = aki_risk * 2.0 - 1.0  # Positive slope = worsening
                    else:  # peak_time
                        values = np.random.uniform(0, 48, n_subjects)
                elif biomarker == "urea":
                    baseline = np.random.normal(20, 8, n_subjects).clip(5, 100)
                    multiplier = {
                        "mean": 1.0,
                        "max": 1.8,
                        "min": 0.6,
                        "std": 0.3,
                        "slope_24h": 1.0,
                        "peak_time": 24.0,
                    }[stat]
                    values = baseline * (1 + aki_risk * multiplier)
                    if stat == "slope_24h":
                        values = values - baseline  # Change from baseline
                    elif stat == "peak_time":
                        values = np.random.uniform(0, 48, n_subjects)
                else:  # electrolytes
                    normal_values = {"potassium": 4.0, "sodium": 140, "chloride": 100}[
                        biomarker
                    ]
                    normal_std = {"potassium": 0.5, "sodium": 5, "chloride": 5}[
                        biomarker
                    ]
                    baseline = np.random.normal(normal_values, normal_std, n_subjects)

                    if stat in ["mean", "min", "max"]:
                        multiplier = {"mean": 1.0, "max": 1.1, "min": 0.9}[stat]
                        values = baseline * multiplier + aki_risk * normal_std * 0.5
                    elif stat == "std":
                        values = normal_std * (0.5 + aki_risk)
                    elif stat == "slope_24h":
                        values = aki_risk * normal_std * 0.3
                    else:  # peak_time
                        values = np.random.uniform(0, 48, n_subjects)

                cohort_data[f"{biomarker}_{stat}"] = values

        # Additional clinical biomarkers
        other_biomarkers = {
            "hemoglobin": (12, 2, -1.5),  # (mean, std, aki_effect)
            "platelets": (250, 80, -50),
            "wbc": (8, 3, 2),
            "glucose": (120, 30, 20),
            "lactate": (1.5, 0.8, 1.5),
            "ast": (30, 15, 25),
            "alt": (25, 12, 20),
            "bilirubin": (1.0, 0.5, 0.8),
        }

        for biomarker, (mean_val, std_val, aki_effect) in other_biomarkers.items():
            for stat in ["min", "max", "mean"]:
                multiplier = {"mean": 1.0, "max": 1.3, "min": 0.7}[stat]
                values = np.random.normal(mean_val, std_val, n_subjects) * multiplier
                values += aki_risk * aki_effect
                cohort_data[f"{biomarker}_{stat}"] = np.maximum(
                    values, 0.1
                )  # Prevent negative values

        # Clinical outcomes
        aki_probability = np.clip(aki_risk * 2, 0, 0.4)  # Max 40% AKI rate
        cohort_data["aki_label"] = np.random.binomial(1, aki_probability, n_subjects)

        # Additional outcomes for multi-outcome analysis
        cohort_data["mortality_30d"] = np.random.binomial(
            1, aki_probability * 0.3, n_subjects
        )
        cohort_data["dialysis_required"] = np.random.binomial(
            1, aki_probability * 0.2 * cohort_data["aki_label"], n_subjects
        )
        cohort_data["los_days"] = np.random.exponential(5, n_subjects) * (
            1 + aki_risk * 2
        )
        cohort_data["recovery_time_hours"] = np.where(
            cohort_data["aki_label"],
            np.random.exponential(72, n_subjects),
            np.random.exponential(24, n_subjects),
        )

        cohort_df = pd.DataFrame(cohort_data)

        logger.info(f"   ‚úÖ Created synthetic cohort with {len(cohort_df)} subjects")
        logger.info(f"   üìä AKI rate: {cohort_df['aki_label'].mean():.1%}")
        logger.info(f"   üìä Mortality rate: {cohort_df['mortality_30d'].mean():.1%}")

        return cohort_df

    def _identify_biomarker_columns(self, df: pd.DataFrame) -> List[str]:
        """Enhanced biomarker column identification for larger datasets"""

        # Known clinical biomarkers with temporal suffixes
        clinical_biomarkers = [
            "creatinine",
            "urea",
            "bun",
            "glucose",
            "sodium",
            "potassium",
            "chloride",
            "co2",
            "hemoglobin",
            "hematocrit",
            "platelets",
            "wbc",
            "neutrophils",
            "lactate",
            "ph",
            "pco2",
            "po2",
            "bicarbonate",
            "ast",
            "alt",
            "bilirubin",
            "albumin",
            "protein",
            "troponin",
            "bnp",
            "ntprobnp",
            "ck",
            "ckmb",
        ]

        # Temporal suffixes
        temporal_suffixes = [
            "_min",
            "_max",
            "_mean",
            "_std",
            "_slope_24h",
            "_peak_time",
            "_mg_dL_min",
            "_mg_dL_max",
            "_mg_dL_mean",
        ]

        # Feature patterns
        biomarker_patterns = [
            "feat_",
            "lab_",
            "bio_",
            "marker_",
            "_mg_dl",
            "_level",
            "_concentration",
            "_value",
            "_result",
            "module_",
            "gene_",
        ]

        identified_cols = []

        for col in df.columns:
            col_lower = col.lower()

            # Skip ID and outcome columns
            if any(
                skip in col_lower
                for skip in [
                    "subject_id",
                    "hadm_id",
                    "aki_label",
                    "mortality",
                    "dialysis",
                    "los_",
                    "recovery_",
                    "age",
                    "gender",
                    "admission",
                ]
            ):
                continue

            # Check temporal biomarkers
            for biomarker in clinical_biomarkers:
                for suffix in temporal_suffixes:
                    if col_lower == f"{biomarker}{suffix}" or col_lower.startswith(
                        f"{biomarker}{suffix}"
                    ):
                        identified_cols.append(col)
                        break
                else:
                    continue
                break
            else:
                # Check patterns
                if any(pattern in col_lower for pattern in biomarker_patterns):
                    identified_cols.append(col)
                    continue

                # Check if numeric with reasonable biomarker range
                if pd.api.types.is_numeric_dtype(df[col]):
                    values = df[col].dropna()
                    if len(values) > 10:  # Enough data points
                        median_val = values.median()
                        if 0.001 <= median_val <= 100000 and values.std() > 0:
                            identified_cols.append(col)

        return identified_cols

    def load_expanded_tubular_modules(self) -> Optional[Dict[str, List[str]]]:
        """Load and expand tubular modules for scaled analysis"""

        logger.info("üß¨ Loading expanded tubular modules...")

        modules = {}

        # Load existing modules
        if self.modules_file.exists():
            try:
                with open(self.modules_file, "r") as f:
                    base_modules = json.load(f)
                modules.update(base_modules)
                logger.info(f"   üìÅ Loaded {len(base_modules)} base modules")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Error loading base modules: {e}")

        # Add expanded pathway modules for comprehensive analysis
        expanded_modules = {
            "acute_kidney_injury": [
                "HAVCR1",
                "LCN2",
                "IL18",
                "CCL2",
                "CXCL1",
                "CXCL2",
                "ICAM1",
                "VCAM1",
                "KIM1",
                "NGAL",
                "L_FABP",
                "NAG",
                "GST",
                "TIMP2",
                "IGFBP7",
            ],
            "tubular_transport": [
                "SLC34A1",
                "SLC5A2",
                "SLC9A3",
                "SLC12A1",
                "SLC12A3",
                "SCNN1A",
                "SCNN1B",
                "SCNN1G",
                "ATP1A1",
                "ATP1B1",
                "ATP2B1",
                "CLCN5",
                "CLCNKA",
                "CLCNKB",
            ],
            "oxidative_stress": [
                "SOD1",
                "SOD2",
                "CAT",
                "GPX1",
                "GPX4",
                "PRDX1",
                "PRDX6",
                "NQO1",
                "HMOX1",
                "NRF2",
            ],
            "inflammation": [
                "TNF",
                "IL1B",
                "IL6",
                "IL10",
                "NLRP3",
                "CASP1",
                "NF-KB1",
                "RELA",
                "TLR4",
                "MYD88",
            ],
            "fibrosis": [
                "COL1A1",
                "COL3A1",
                "COL4A1",
                "FN1",
                "ACTA2",
                "TGFB1",
                "SMAD3",
                "CTGF",
                "PDGFRA",
            ],
            "apoptosis": [
                "BAX",
                "BCL2",
                "CASP3",
                "CASP9",
                "PARP1",
                "TP53",
                "MDM2",
                "PUMA",
                "NOXA",
            ],
            "metabolism": [
                "HK1",
                "PFKM",
                "LDHA",
                "PDK1",
                "PCK1",
                "G6PC",
                "PEPCK",
                "CPT1A",
                "ACOX1",
            ],
        }

        modules.update(expanded_modules)

        logger.info(f"   ‚úÖ Total modules available: {len(modules)}")
        for module_name, genes in expanded_modules.items():
            logger.info(f"      {module_name}: {len(genes)} genes")

        return modules

    def create_scaled_molecular_features(
        self, modules: Dict[str, List[str]], n_subjects: int
    ) -> pd.DataFrame:
        """Create molecular features for scaled analysis with realistic correlations"""

        logger.info(
            f"üß¨ Creating scaled molecular features for {n_subjects} subjects..."
        )

        np.random.seed(42)

        # Create subject IDs that match clinical data format
        subject_ids = [f"scaled_subj_{i:06d}" for i in range(n_subjects)]

        molecular_data = pd.DataFrame({"subject_id": subject_ids})

        # Create disease state for realistic correlations
        disease_severity = np.random.beta(2, 5, n_subjects)  # Most healthy, some sick

        # Module-level features with realistic pathway interactions
        for module_name, genes in modules.items():
            if len(genes) == 0:
                continue

            # Base activity level depends on pathway type
            if any(
                keyword in module_name.lower()
                for keyword in ["injury", "inflammation", "fibrosis", "apoptosis"]
            ):
                # Disease-associated pathways - higher in disease
                base_activity = np.random.gamma(2, 2, n_subjects) * 10
                disease_effect = disease_severity * 30
            elif any(
                keyword in module_name.lower()
                for keyword in ["transport", "metabolism"]
            ):
                # Functional pathways - lower in disease
                base_activity = np.random.gamma(5, 3, n_subjects) * 10
                disease_effect = -disease_severity * 20
            elif "repair" in module_name.lower():
                # Repair pathways - variable response
                base_activity = np.random.gamma(3, 3, n_subjects) * 10
                disease_effect = (
                    disease_severity * 15 * np.random.choice([-1, 1], n_subjects)
                )
            else:
                # Default
                base_activity = np.random.gamma(4, 2.5, n_subjects) * 10
                disease_effect = disease_severity * 10

            module_activity = (
                base_activity + disease_effect + np.random.normal(0, 5, n_subjects)
            )
            module_activity = np.maximum(module_activity, 0)  # Non-negative

            molecular_data[f"module_{module_name}"] = module_activity

        # Individual gene features for top biomarker genes
        top_genes = [
            "HAVCR1",
            "LCN2",
            "IL18",
            "TIMP2",
            "IGFBP7",
            "TNF",
            "IL6",
            "TGFB1",
            "COL1A1",
            "SOD2",
        ]

        for gene in top_genes:
            # Find parent modules for this gene
            parent_modules = [
                mod for mod, gene_list in modules.items() if gene in gene_list
            ]

            if parent_modules:
                # Base expression on parent module activity
                parent_module = parent_modules[0]
                if f"module_{parent_module}" in molecular_data.columns:
                    base_expression = molecular_data[
                        f"module_{parent_module}"
                    ] * np.random.uniform(0.8, 1.2, n_subjects)
                    noise = np.random.normal(0, base_expression.std() * 0.3, n_subjects)
                    gene_expression = base_expression + noise
                    molecular_data[f"gene_{gene}"] = np.maximum(gene_expression, 0)

        # Add pathway interaction features
        if (
            "module_inflammation" in molecular_data.columns
            and "module_fibrosis" in molecular_data.columns
        ):
            molecular_data["pathway_inflammation_fibrosis"] = (
                molecular_data["module_inflammation"]
                * molecular_data["module_fibrosis"]
                / 100
            )

        if (
            "module_oxidative_stress" in molecular_data.columns
            and "module_apoptosis" in molecular_data.columns
        ):
            molecular_data["pathway_oxidative_apoptosis"] = (
                molecular_data["module_oxidative_stress"]
                * molecular_data["module_apoptosis"]
                / 100
            )

        logger.info(f"   ‚úÖ Created {len(molecular_data.columns)-1} molecular features")
        logger.info(
            f"   üìä Module features: {len([c for c in molecular_data.columns if c.startswith('module_')])}"
        )
        logger.info(
            f"   üìä Gene features: {len([c for c in molecular_data.columns if c.startswith('gene_')])}"
        )
        logger.info(
            f"   üìä Pathway interactions: {len([c for c in molecular_data.columns if c.startswith('pathway_')])}"
        )

        return molecular_data

    def run_scaled_causal_analysis(
        self,
        cohort_data: pd.DataFrame,
        molecular_data: pd.DataFrame,
        outcomes: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Run causal analysis on scaled data with parallel processing"""

        outcomes = outcomes or ["aki_label", "mortality_30d", "dialysis_required"]

        logger.info("üî¨ Running scaled causal analysis...")
        logger.info(f"   üìä Cohort size: {len(cohort_data)}")
        logger.info(f"   üìä Outcomes: {', '.join(outcomes)}")

        # Merge clinical and molecular data
        combined_data = cohort_data.merge(molecular_data, on="subject_id", how="inner")
        logger.info(f"   üìä Combined dataset: {len(combined_data)} subjects")

        # Identify biomarker columns
        biomarker_columns = [
            col
            for col in combined_data.columns
            if col
            not in ["subject_id"]
            + outcomes
            + ["age", "gender", "admission_type", "los_days", "recovery_time_hours"]
        ]

        logger.info(f"   üìä Total biomarkers: {len(biomarker_columns)}")

        results = {}

        # Process each outcome
        for outcome in outcomes:
            if outcome in combined_data.columns:
                logger.info(f"\nüéØ Analyzing outcome: {outcome}")

                # Prepare data for this outcome
                outcome_data = combined_data[[outcome] + biomarker_columns].dropna()

                if len(outcome_data) < 100:
                    logger.warning(
                        f"   ‚ö†Ô∏è Insufficient data for {outcome}: {len(outcome_data)} subjects"
                    )
                    continue

                # Create biomarker metadata
                biomarker_metadata = pd.DataFrame(
                    {
                        "layer": [
                            (
                                "clinical"
                                if any(
                                    term in col.lower()
                                    for term in [
                                        "creatinine",
                                        "urea",
                                        "sodium",
                                        "glucose",
                                        "hemoglobin",
                                        "platelets",
                                        "wbc",
                                    ]
                                )
                                else "molecular"
                            )
                            for col in biomarker_columns
                        ],
                        "type": [
                            (
                                "lab"
                                if any(
                                    term in col.lower()
                                    for term in [
                                        "_min",
                                        "_max",
                                        "_mean",
                                        "_std",
                                        "_slope",
                                    ]
                                )
                                else (
                                    "module"
                                    if col.startswith("module_")
                                    else (
                                        "gene" if col.startswith("gene_") else "pathway"
                                    )
                                )
                            )
                            for col in biomarker_columns
                        ],
                    },
                    index=biomarker_columns,
                )

                # Split data for analysis
                biomarker_data = outcome_data[biomarker_columns]
                clinical_outcomes = outcome_data[[outcome]].rename(
                    columns={outcome: "outcome"}
                )

                # Initialize scorer
                scorer = CausalBiomarkerScorer()

                # Run analysis
                try:
                    scored_biomarkers = scorer.discover_and_score_biomarkers(
                        biomarker_data=biomarker_data,
                        clinical_outcomes=clinical_outcomes,
                        biomarker_metadata=biomarker_metadata,
                        outcome_column="outcome",
                    )

                    results[outcome] = {
                        "scored_biomarkers": scored_biomarkers,
                        "scorer": scorer,
                        "n_subjects": len(outcome_data),
                        "n_biomarkers": len(biomarker_columns),
                        "outcome_rate": clinical_outcomes["outcome"].mean(),
                    }

                    logger.info(
                        f"   ‚úÖ Analyzed {len(scored_biomarkers)} biomarkers for {outcome}"
                    )
                    logger.info(
                        f"   üèÜ Top biomarker: {scored_biomarkers[0].name} (score: {scored_biomarkers[0].integrated_score:.3f})"
                    )

                except Exception as e:
                    logger.error(f"   ‚ùå Error analyzing {outcome}: {e}")
                    continue

        return results

    def export_scaled_results(self, results: Dict[str, Any], output_dir: Path) -> None:
        """Export comprehensive results from scaled analysis"""

        logger.info("üìÇ Exporting scaled analysis results...")

        output_dir.mkdir(parents=True, exist_ok=True)

        # Create comprehensive summary
        summary_report = f"""# Scaled Causal Biomarker Analysis Report

Generated: {datetime.now().isoformat()}

## Analysis Overview

"""

        for outcome, outcome_results in results.items():
            scored_biomarkers = outcome_results["scored_biomarkers"]
            n_subjects = outcome_results["n_subjects"]
            n_biomarkers = outcome_results["n_biomarkers"]
            outcome_rate = outcome_results["outcome_rate"]

            summary_report += f"""
### {outcome.upper()} Analysis

- **Subjects Analyzed**: {n_subjects:,}
- **Biomarkers Analyzed**: {n_biomarkers}
- **Outcome Rate**: {outcome_rate:.1%}
- **Top Biomarker**: {scored_biomarkers[0].name} (score: {scored_biomarkers[0].integrated_score:.3f})

#### Top 10 Biomarkers for {outcome}

| Rank | Biomarker | Score | Confidence | Evidence Tier | Layer |
|------|-----------|-------|------------|---------------|-------|
"""

            for i, score in enumerate(scored_biomarkers[:10], 1):
                summary_report += f"| {i} | {score.name} | {score.integrated_score:.3f} | {score.causal_confidence:.3f} | {score.evidence_tier} | {score.layer} |\n"

            # Export individual results
            outcome_dir = output_dir / f"outcome_{outcome}"
            outcome_dir.mkdir(exist_ok=True)

            # Export scored biomarkers
            scorer = outcome_results["scorer"]
            scorer.export_scored_biomarkers(scored_biomarkers, outcome_dir)

        # Save summary report
        with open(output_dir / "scaled_analysis_summary.md", "w") as f:
            f.write(summary_report)

        # Create cross-outcome comparison
        self._create_cross_outcome_analysis(results, output_dir)

        logger.info(f"   ‚úÖ Results exported to: {output_dir}")

    def _create_cross_outcome_analysis(
        self, results: Dict[str, Any], output_dir: Path
    ) -> None:
        """Create analysis comparing biomarkers across outcomes"""

        logger.info("üîç Creating cross-outcome biomarker analysis...")

        # Collect all biomarkers across outcomes
        all_biomarkers = {}

        for outcome, outcome_results in results.items():
            for score in outcome_results["scored_biomarkers"]:
                if score.name not in all_biomarkers:
                    all_biomarkers[score.name] = {}
                all_biomarkers[score.name][outcome] = {
                    "score": score.integrated_score,
                    "confidence": score.causal_confidence,
                    "tier": score.evidence_tier,
                    "layer": score.layer,
                }

        # Create cross-outcome dataframe
        cross_outcome_data = []
        for biomarker, outcome_scores in all_biomarkers.items():
            row = {"biomarker": biomarker}
            for outcome in results.keys():
                if outcome in outcome_scores:
                    row[f"{outcome}_score"] = outcome_scores[outcome]["score"]
                    row[f"{outcome}_confidence"] = outcome_scores[outcome]["confidence"]
                    row[f"{outcome}_tier"] = outcome_scores[outcome]["tier"]
                else:
                    row[f"{outcome}_score"] = 0.0
                    row[f"{outcome}_confidence"] = 0.0
                    row[f"{outcome}_tier"] = 5

            # Add aggregate metrics
            scores = [row[f"{outcome}_score"] for outcome in results.keys()]
            row["mean_score"] = np.mean(scores)
            row["max_score"] = np.max(scores)
            row["score_consistency"] = 1.0 - np.std(scores) / (np.mean(scores) + 1e-8)

            if len(outcome_scores) > 0:
                row["layer"] = list(outcome_scores.values())[0]["layer"]
            else:
                row["layer"] = "unknown"

            cross_outcome_data.append(row)

        cross_df = pd.DataFrame(cross_outcome_data)
        cross_df = cross_df.sort_values("mean_score", ascending=False)

        # Export cross-outcome analysis
        cross_df.to_csv(output_dir / "cross_outcome_biomarkers.csv", index=False)

        # Create cross-outcome report
        cross_report = """# Cross-Outcome Biomarker Analysis

## Multi-Outcome Biomarkers

### Top 20 Biomarkers by Mean Score Across All Outcomes

| Rank | Biomarker | Mean Score | Max Score | Consistency | Layer |
|------|-----------|------------|-----------|-------------|-------|
"""

        for i, (_, row) in enumerate(cross_df.head(20).iterrows(), 1):
            cross_report += f"| {i} | {row['biomarker']} | {row['mean_score']:.3f} | {row['max_score']:.3f} | {row['score_consistency']:.3f} | {row['layer']} |\n"

        cross_report += """

## Outcome-Specific vs Universal Biomarkers

### Universal Biomarkers (High scores across multiple outcomes)
"""

        # Find universal biomarkers
        universal_threshold = 0.3
        universal_biomarkers = cross_df[
            (cross_df["mean_score"] > universal_threshold)
            & (cross_df["score_consistency"] > 0.5)
        ].head(10)

        for _, row in universal_biomarkers.iterrows():
            cross_report += f"- **{row['biomarker']}** (mean: {row['mean_score']:.3f}, consistency: {row['score_consistency']:.3f})\n"

        # Save cross-outcome report
        with open(output_dir / "cross_outcome_analysis.md", "w") as f:
            f.write(cross_report)

        logger.info("   ‚úÖ Cross-outcome analysis completed")


def run_scaled_causal_analysis():
    """
    Main function to run scaled causal analysis on full cohort
    """
    logger.info("üöÄ SCALED CAUSAL BIOMARKER ANALYSIS")
    logger.info("=" * 60)

    # Configuration for scaled analysis
    config = CohortConfiguration(
        min_subjects=2000,
        max_subjects=5000,
        min_biomarkers=20,
        temporal_window_hours=48,
        parallel_processing=True,
        max_workers=4,
        chunk_size=500,
        cache_results=True,
    )

    # Initialize scaled connector
    connector = ScaledBiomarkerDataConnector(config=config)

    try:
        # Load full cohort
        cohort_data = connector.load_full_mimic_cohort()
        if cohort_data is None:
            raise ValueError("Failed to load cohort data")

        # Load expanded molecular features
        modules = connector.load_expanded_tubular_modules()
        if modules is None:
            raise ValueError("Failed to load tubular modules")

        # Create molecular features
        molecular_data = connector.create_scaled_molecular_features(
            modules, len(cohort_data)
        )

        # Run scaled causal analysis
        outcomes = ["aki_label", "mortality_30d", "dialysis_required"]
        results = connector.run_scaled_causal_analysis(
            cohort_data, molecular_data, outcomes
        )

        # Export results
        output_dir = Path("artifacts/scaled_analysis")
        connector.export_scaled_results(results, output_dir)

        # Create visualizations if available
        try:
            from biomarkers.visualization import CausalGraphVisualizer
            import matplotlib

            matplotlib.use("Agg")

            visualizer = CausalGraphVisualizer()

            # Create visualizations for each outcome
            for outcome, outcome_results in results.items():
                viz_dir = output_dir / f"outcome_{outcome}" / "visualizations"
                viz_dir.mkdir(parents=True, exist_ok=True)

                scorer = outcome_results["scorer"]
                scored_biomarkers = outcome_results["scored_biomarkers"]

                if scorer.causal_graph and scorer.networkx_graph:
                    visualizer.create_biomarker_dashboard(
                        scored_biomarkers=scored_biomarkers,
                        causal_graph=scorer.causal_graph,
                        networkx_graph=scorer.networkx_graph,
                        output_dir=viz_dir,
                    )
        except ImportError:
            logger.warning("üìä Visualization libraries not available")

        # Summary
        total_subjects = len(cohort_data)
        total_outcomes = len(results)
        logger.info("\nüéâ SCALED ANALYSIS COMPLETE!")
        logger.info(f"üìà Analyzed {total_subjects:,} subjects")
        logger.info(f"üéØ Analyzed {total_outcomes} clinical outcomes")
        logger.info(f"üìÇ Results saved to: {output_dir}")

        # Print top findings
        for outcome, outcome_results in results.items():
            top_biomarker = outcome_results["scored_biomarkers"][0]
            logger.info(
                f"üèÜ {outcome}: {top_biomarker.name} (score: {top_biomarker.integrated_score:.3f})"
            )

        return results

    except Exception as e:
        logger.error(f"‚ùå Error in scaled analysis: {e}")
        raise


if __name__ == "__main__":
    run_scaled_causal_analysis()
